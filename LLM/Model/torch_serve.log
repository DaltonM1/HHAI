WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.
2024-04-17T00:04:49,582 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2024-04-17T00:04:49,584 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2024-04-17T00:04:49,614 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /home/students/mdr2104/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml
2024-04-17T00:04:49,713 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.10.0
TS Home: /home/students/mdr2104/.local/lib/python3.10/site-packages
Current directory: /opt/HHAI/LLM/Model
Temp directory: /tmp
Metrics config path: /home/students/mdr2104/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 32
Max heap size: 16004 M
Python executable: /usr/bin/python3
Config file: config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /opt/HHAI/LLM/Model
Initial Models: wizardlmtest.mar
Log dir: /opt/HHAI/LLM/Model/logs
Metrics dir: /opt/HHAI/LLM/Model/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: false
Workflow Store: /opt/HHAI/LLM/Model
CPP log config: N/A
Model config: N/A
System metrics command: default
2024-04-17T00:04:49,722 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2024-04-17T00:04:49,731 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: wizardlmtest.mar
2024-04-17T00:05:29,194 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model wizardlmtest
2024-04-17T00:05:29,194 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model wizardlmtest
2024-04-17T00:05:29,194 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model wizardlmtest loaded.
2024-04-17T00:05:29,194 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: wizardlmtest, count: 1
2024-04-17T00:05:29,199 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2024-04-17T00:05:29,198 [DEBUG] W-9000-wizardlmtest_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - launcherAvailable cmdline: [python, -m, torch.backends.xeon.run_cpu, --use_logical_core, --no_python, hostname]
2024-04-17T00:05:29,226 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2024-04-17T00:05:29,226 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2024-04-17T00:05:29,226 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2024-04-17T00:05:29,227 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2024-04-17T00:05:29,227 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
Model server started.
2024-04-17T00:05:29,320 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2024-04-17T00:05:29,363 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/students/mdr2104/.local/lib/python3.10/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/students/mdr2104/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/students/mdr2104/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 66, in gpu_utilization
    from nvgpu import list_gpus
  File "/home/students/mdr2104/.local/lib/python3.10/site-packages/nvgpu/list_gpus.py", line 3, in <module>
    import arrow
ModuleNotFoundError: No module named 'arrow'

2024-04-17T00:05:29,476 [WARN ] W-9000-wizardlmtest_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - torch.backends.xeon.run_cpu is not available. Proceeding without worker core pinning. For better performance, please make sure torch.backends.xeon.run_cpu is available.
2024-04-17T00:05:29,477 [DEBUG] W-9000-wizardlmtest_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /home/students/mdr2104/.local/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/students/mdr2104/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2024-04-17T00:05:29,959 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=965574
2024-04-17T00:05:29,959 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2024-04-17T00:05:29,962 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - Successfully loaded /home/students/mdr2104/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2024-04-17T00:05:29,962 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - [PID]965574
2024-04-17T00:05:29,962 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - Torch worker started.
2024-04-17T00:05:29,962 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - Python runtime: 3.10.12
2024-04-17T00:05:29,962 [DEBUG] W-9000-wizardlmtest_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-wizardlmtest_1.0 State change null -> WORKER_STARTED
2024-04-17T00:05:29,964 [INFO ] W-9000-wizardlmtest_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2024-04-17T00:05:29,968 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2024-04-17T00:05:29,969 [DEBUG] W-9000-wizardlmtest_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1713312329969
2024-04-17T00:05:29,970 [INFO ] W-9000-wizardlmtest_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1713312329970
2024-04-17T00:05:29,982 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - model_name: wizardlmtest, batchSize: 1
2024-04-17T00:05:30,049 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - Enabled tensor cores
2024-04-17T00:05:30,049 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - proceeding without onnxruntime
2024-04-17T00:05:30,049 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2024-04-17T00:05:30,370 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llama_model_loader: loaded meta data with 19 key-value pairs and 363 tensors from /tmp/models/33f4715239304a8bbccec6b7b83295d4/wizardlm-13b-v1.2.Q4_0.gguf (version GGUF V2)
2024-04-17T00:05:30,371 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
2024-04-17T00:05:30,371 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llama_model_loader: - kv   0:                       general.architecture str              = llama
2024-04-17T00:05:30,371 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llama_model_loader: - kv   1:                               general.name str              = LLaMA v2
2024-04-17T00:05:30,371 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llama_model_loader: - kv   2:                       llama.context_length u32              = 4096
2024-04-17T00:05:30,371 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120
2024-04-17T00:05:30,371 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llama_model_loader: - kv   4:                          llama.block_count u32              = 40
2024-04-17T00:05:30,371 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824
2024-04-17T00:05:30,371 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
2024-04-17T00:05:30,371 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40
2024-04-17T00:05:30,371 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40
2024-04-17T00:05:30,371 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
2024-04-17T00:05:30,371 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llama_model_loader: - kv  10:                          general.file_type u32              = 2
2024-04-17T00:05:30,371 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
2024-04-17T00:05:30,372 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
2024-04-17T00:05:30,377 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
2024-04-17T00:05:30,378 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
2024-04-17T00:05:30,378 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
2024-04-17T00:05:30,378 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
2024-04-17T00:05:30,378 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0
2024-04-17T00:05:30,378 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llama_model_loader: - kv  18:               general.quantization_version u32              = 2
2024-04-17T00:05:30,378 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llama_model_loader: - type  f32:   81 tensors
2024-04-17T00:05:30,378 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llama_model_loader: - type q4_0:  281 tensors
2024-04-17T00:05:30,378 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llama_model_loader: - type q6_K:    1 tensors
2024-04-17T00:05:30,384 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llm_load_vocab: special tokens definition check successful ( 259/32000 ).
2024-04-17T00:05:30,384 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llm_load_print_meta: format           = GGUF V2
2024-04-17T00:05:30,384 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llm_load_print_meta: arch             = llama
2024-04-17T00:05:30,384 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llm_load_print_meta: vocab type       = SPM
2024-04-17T00:05:30,384 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llm_load_print_meta: n_vocab          = 32000
2024-04-17T00:05:30,385 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llm_load_print_meta: n_merges         = 0
2024-04-17T00:05:30,385 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llm_load_print_meta: n_ctx_train      = 4096
2024-04-17T00:05:30,385 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llm_load_print_meta: n_embd           = 5120
2024-04-17T00:05:30,385 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llm_load_print_meta: n_head           = 40
2024-04-17T00:05:30,385 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llm_load_print_meta: n_head_kv        = 40
2024-04-17T00:05:30,385 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llm_load_print_meta: n_layer          = 40
2024-04-17T00:05:30,385 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llm_load_print_meta: n_rot            = 128
2024-04-17T00:05:30,385 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llm_load_print_meta: n_embd_head_k    = 128
2024-04-17T00:05:30,385 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llm_load_print_meta: n_embd_head_v    = 128
2024-04-17T00:05:30,385 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llm_load_print_meta: n_gqa            = 1
2024-04-17T00:05:30,385 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llm_load_print_meta: n_embd_k_gqa     = 5120
2024-04-17T00:05:30,385 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llm_load_print_meta: n_embd_v_gqa     = 5120
2024-04-17T00:05:30,385 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llm_load_print_meta: f_norm_eps       = 0.0e+00
2024-04-17T00:05:30,385 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
2024-04-17T00:05:30,385 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llm_load_print_meta: f_clamp_kqv      = 0.0e+00
2024-04-17T00:05:30,385 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llm_load_print_meta: f_max_alibi_bias = 0.0e+00
2024-04-17T00:05:30,385 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llm_load_print_meta: n_ff             = 13824
2024-04-17T00:05:30,385 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llm_load_print_meta: n_expert         = 0
2024-04-17T00:05:30,385 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llm_load_print_meta: n_expert_used    = 0
2024-04-17T00:05:30,385 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llm_load_print_meta: rope scaling     = linear
2024-04-17T00:05:30,385 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llm_load_print_meta: freq_base_train  = 10000.0
2024-04-17T00:05:30,385 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llm_load_print_meta: freq_scale_train = 1
2024-04-17T00:05:30,385 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llm_load_print_meta: n_yarn_orig_ctx  = 4096
2024-04-17T00:05:30,385 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llm_load_print_meta: rope_finetuned   = unknown
2024-04-17T00:05:30,385 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llm_load_print_meta: model type       = 13B
2024-04-17T00:05:30,385 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llm_load_print_meta: model ftype      = Q4_0
2024-04-17T00:05:30,386 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llm_load_print_meta: model params     = 13.02 B
2024-04-17T00:05:30,386 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llm_load_print_meta: model size       = 6.86 GiB (4.53 BPW) 
2024-04-17T00:05:30,386 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llm_load_print_meta: general.name     = LLaMA v2
2024-04-17T00:05:30,386 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llm_load_print_meta: BOS token        = 1 '<s>'
2024-04-17T00:05:30,386 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llm_load_print_meta: EOS token        = 2 '</s>'
2024-04-17T00:05:30,386 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llm_load_print_meta: UNK token        = 0 '<unk>'
2024-04-17T00:05:30,386 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llm_load_print_meta: LF token         = 13 '<0x0A>'
2024-04-17T00:05:30,386 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llm_load_tensors: ggml ctx size =    0.14 MiB
2024-04-17T00:05:30,500 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llm_load_tensors:        CPU buffer size =  7023.90 MiB
2024-04-17T00:05:30,500 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - ...................................................................................................
2024-04-17T00:05:30,500 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llama_new_context_with_model: n_ctx      = 512
2024-04-17T00:05:30,500 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llama_new_context_with_model: freq_base  = 10000.0
2024-04-17T00:05:30,500 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llama_new_context_with_model: freq_scale = 1
2024-04-17T00:05:30,577 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llama_kv_cache_init:        CPU KV buffer size =   400.00 MiB
2024-04-17T00:05:30,577 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llama_new_context_with_model: KV self size  =  400.00 MiB, K (f16):  200.00 MiB, V (f16):  200.00 MiB
2024-04-17T00:05:30,577 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llama_new_context_with_model:        CPU input buffer size   =    12.01 MiB
2024-04-17T00:05:30,579 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llama_new_context_with_model:        CPU compute buffer size =    80.00 MiB
2024-04-17T00:05:30,579 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llama_new_context_with_model: graph splits (measure): 1
2024-04-17T00:05:30,579 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | 
2024-04-17T00:05:30,579 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '5120', 'llama.feed_forward_length': '13824', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '40', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '40', 'llama.attention.head_count_kv': '40', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '2'}
2024-04-17T00:05:30,580 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - Loading faiss with AVX2 support.
2024-04-17T00:05:30,580 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - Could not load library with AVX2 support due to:
2024-04-17T00:05:30,580 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - ModuleNotFoundError("No module named 'faiss.swigfaiss_avx2'")
2024-04-17T00:05:30,580 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - Loading faiss.
2024-04-17T00:05:30,587 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - Successfully loaded faiss.
2024-04-17T00:05:33,107 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llama.cpp: using Vulkan on NVIDIA GeForce RTX 4090
2024-04-17T00:05:33,107 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - Model path: /tmp/models/33f4715239304a8bbccec6b7b83295d4/wizardlm-13b-v1.2.Q4_0.gguf
2024-04-17T00:05:33,107 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - Index path: /tmp/models/33f4715239304a8bbccec6b7b83295d4
2024-04-17T00:05:33,108 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - Device info: NVIDIA GeForce RTX 4090
2024-04-17T00:05:33,111 [INFO ] W-9000-wizardlmtest_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3141
2024-04-17T00:05:33,111 [DEBUG] W-9000-wizardlmtest_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-wizardlmtest_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2024-04-17T00:05:33,112 [INFO ] W-9000-wizardlmtest_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:3915.0|#WorkerName:W-9000-wizardlmtest_1.0,Level:Host|#hostname:ai,timestamp:1713312333
2024-04-17T00:05:33,112 [INFO ] W-9000-wizardlmtest_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:2.0|#Level:Host|#hostname:ai,timestamp:1713312333
2024-04-17T00:06:07,928 [INFO ] epollEventLoopGroup-3-1 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:wizardlmtest,model_version:default|#hostname:ai,timestamp:1713312367
2024-04-17T00:06:07,929 [DEBUG] W-9000-wizardlmtest_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT repeats 1 to backend at: 1713312367929
2024-04-17T00:06:07,929 [INFO ] W-9000-wizardlmtest_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1713312367929
2024-04-17T00:06:07,930 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - Backend received inference at: 1713312367
2024-04-17T00:06:08,112 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - 
2024-04-17T00:06:08,112 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llama_print_timings:        load time =     180.03 ms
2024-04-17T00:06:08,112 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
2024-04-17T00:06:08,112 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llama_print_timings: prompt eval time =     179.97 ms /     2 tokens (   89.98 ms per token,    11.11 tokens per second)
2024-04-17T00:06:08,112 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
2024-04-17T00:06:08,112 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llama_print_timings:       total time =     179.88 ms /     3 tokens
2024-04-17T00:06:11,373 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - 
2024-04-17T00:06:11,373 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - 
2024-04-17T00:06:11,373 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - [1m> Entering new StuffDocumentsChain chain...[0m
2024-04-17T00:06:11,373 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - 
2024-04-17T00:06:11,373 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - 
2024-04-17T00:06:11,374 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - [1m> Entering new LLMChain chain...[0m
2024-04-17T00:06:11,374 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - Prompt after formatting:
2024-04-17T00:06:11,374 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - [32;1m[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.
2024-04-17T00:06:11,374 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - 
2024-04-17T00:06:11,374 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - one retake for a better grade. Because they previously passed the course, they would not be able to take for a third time and receive aid. Q: Can a student repeat a WP? A course from which a student withdraws does not count as a repetition of a
2024-04-17T00:06:11,375 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - 
2024-04-17T00:06:11,375 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - and what does that mean? Yes, HSU does have a rolling admission process.
2024-04-17T00:06:11,375 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - 
2024-04-17T00:06:11,375 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - can't wait to see you at HSU! Click the button below and get started on your application. If you have any questions, you can call Admissions at 877-GO-HSUTX. The best way to really experience Hardin-Simmons University is through a campus visit. We host a
2024-04-17T00:06:11,375 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - 
2024-04-17T00:06:11,375 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - can't wait to see you at HSU! Click the button below and get started on your application. If you have any questions, you can call Admissions at 877-GO-HSUTX. The best way to really experience Hardin-Simmons University is through a campus visit. We host a
2024-04-17T00:06:11,375 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - 
2024-04-17T00:06:11,375 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - Question: hello
2024-04-17T00:06:11,375 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - Helpful Answer:[0m
2024-04-17T00:06:11,375 [INFO ] W-9000-wizardlmtest_1.0 ACCESS_LOG - /127.0.0.1:56714 "POST /predictions/wizardlmtest HTTP/1.1" 200 3447
2024-04-17T00:06:11,376 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - 
2024-04-17T00:06:11,376 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - [1m> Finished chain.[0m
2024-04-17T00:06:11,376 [INFO ] W-9000-wizardlmtest_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:ai,timestamp:1713312371
2024-04-17T00:06:11,376 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - 
2024-04-17T00:06:11,376 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - [1m> Finished chain.[0m
2024-04-17T00:06:11,376 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - Chat History Updated for User 4um2xl: [('hello', ' Hello! How may I assist you today?')]
2024-04-17T00:06:11,376 [INFO ] W-9000-wizardlmtest_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]HandlerTime.Milliseconds:3442.66|#ModelName:wizardlmtest,Level:Model|#type:GAUGE|#hostname:ai,1713312371,74fc75f3-156b-4c77-9f9b-148b4d6f94cd, pattern=[METRICS]
2024-04-17T00:06:11,376 [INFO ] W-9000-wizardlmtest_1.0 TS_METRICS - ts_inference_latency_microseconds.Microseconds:3445110.981|#model_name:wizardlmtest,model_version:default|#hostname:ai,timestamp:1713312371
2024-04-17T00:06:11,376 [INFO ] W-9000-wizardlmtest_1.0 TS_METRICS - ts_queue_latency_microseconds.Microseconds:113.17|#model_name:wizardlmtest,model_version:default|#hostname:ai,timestamp:1713312371
2024-04-17T00:06:11,376 [DEBUG] W-9000-wizardlmtest_1.0 org.pytorch.serve.job.RestJob - Waiting time ns: 113170, Backend time ns: 3447192326
2024-04-17T00:06:11,377 [INFO ] W-9000-wizardlmtest_1.0 TS_METRICS - QueueTime.Milliseconds:0.0|#Level:Host|#hostname:ai,timestamp:1713312371
2024-04-17T00:06:11,377 [INFO ] W-9000-wizardlmtest_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3445
2024-04-17T00:06:11,377 [INFO ] W-9000-wizardlmtest_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:3.0|#Level:Host|#hostname:ai,timestamp:1713312371
2024-04-17T00:06:11,378 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_METRICS - HandlerTime.ms:3442.66|#ModelName:wizardlmtest,Level:Model|#hostname:ai,requestID:74fc75f3-156b-4c77-9f9b-148b4d6f94cd,timestamp:1713312371
2024-04-17T00:06:11,378 [INFO ] W-9000-wizardlmtest_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:3442.82|#ModelName:wizardlmtest,Level:Model|#type:GAUGE|#hostname:ai,1713312371,74fc75f3-156b-4c77-9f9b-148b4d6f94cd, pattern=[METRICS]
2024-04-17T00:06:11,378 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_METRICS - PredictionTime.ms:3442.82|#ModelName:wizardlmtest,Level:Model|#hostname:ai,requestID:74fc75f3-156b-4c77-9f9b-148b4d6f94cd,timestamp:1713312371
2024-04-17T00:06:29,372 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/students/mdr2104/.local/lib/python3.10/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/students/mdr2104/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/students/mdr2104/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 66, in gpu_utilization
    from nvgpu import list_gpus
  File "/home/students/mdr2104/.local/lib/python3.10/site-packages/nvgpu/list_gpus.py", line 3, in <module>
    import arrow
ModuleNotFoundError: No module named 'arrow'

2024-04-17T00:07:29,375 [ERROR] Thread-3 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/students/mdr2104/.local/lib/python3.10/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/students/mdr2104/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/students/mdr2104/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 66, in gpu_utilization
    from nvgpu import list_gpus
  File "/home/students/mdr2104/.local/lib/python3.10/site-packages/nvgpu/list_gpus.py", line 3, in <module>
    import arrow
ModuleNotFoundError: No module named 'arrow'

2024-04-17T00:08:29,380 [ERROR] Thread-4 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/students/mdr2104/.local/lib/python3.10/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/students/mdr2104/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/students/mdr2104/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 66, in gpu_utilization
    from nvgpu import list_gpus
  File "/home/students/mdr2104/.local/lib/python3.10/site-packages/nvgpu/list_gpus.py", line 3, in <module>
    import arrow
ModuleNotFoundError: No module named 'arrow'

2024-04-17T00:09:29,375 [ERROR] Thread-5 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/students/mdr2104/.local/lib/python3.10/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/students/mdr2104/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/students/mdr2104/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 66, in gpu_utilization
    from nvgpu import list_gpus
  File "/home/students/mdr2104/.local/lib/python3.10/site-packages/nvgpu/list_gpus.py", line 3, in <module>
    import arrow
ModuleNotFoundError: No module named 'arrow'

2024-04-17T00:10:15,473 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:wizardlmtest,model_version:default|#hostname:ai,timestamp:1713312615
2024-04-17T00:10:15,474 [DEBUG] W-9000-wizardlmtest_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT repeats 1 to backend at: 1713312615474
2024-04-17T00:10:15,474 [INFO ] W-9000-wizardlmtest_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1713312615474
2024-04-17T00:10:15,475 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - Backend received inference at: 1713312615
2024-04-17T00:10:17,256 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - 
2024-04-17T00:10:17,256 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llama_print_timings:        load time =     180.03 ms
2024-04-17T00:10:17,256 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
2024-04-17T00:10:17,257 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llama_print_timings: prompt eval time =     694.65 ms /    11 tokens (   63.15 ms per token,    15.84 tokens per second)
2024-04-17T00:10:17,257 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
2024-04-17T00:10:17,257 [WARN ] W-9000-wizardlmtest_1.0-stderr MODEL_LOG - llama_print_timings:       total time =     694.60 ms /    12 tokens
2024-04-17T00:10:20,668 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - 
2024-04-17T00:10:20,669 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - 
2024-04-17T00:10:20,669 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - [1m> Entering new LLMChain chain...[0m
2024-04-17T00:10:20,669 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - Prompt after formatting:
2024-04-17T00:10:20,669 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - [32;1m[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.
2024-04-17T00:10:20,669 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - 
2024-04-17T00:10:20,669 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - Chat History:
2024-04-17T00:10:20,669 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - 
2024-04-17T00:10:20,669 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - Human: hello
2024-04-17T00:10:20,669 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - Assistant:  Hello! How may I assist you today?
2024-04-17T00:10:20,669 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - Follow Up Input: what does HSU stand for?
2024-04-17T00:10:20,669 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - Standalone question:[0m
2024-04-17T00:10:20,669 [INFO ] W-9000-wizardlmtest_1.0 ACCESS_LOG - /127.0.0.1:57572 "POST /predictions/wizardlmtest HTTP/1.1" 200 5196
2024-04-17T00:10:20,669 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - 
2024-04-17T00:10:20,669 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - [1m> Finished chain.[0m
2024-04-17T00:10:20,670 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - 
2024-04-17T00:10:20,670 [INFO ] W-9000-wizardlmtest_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:ai,timestamp:1713312620
2024-04-17T00:10:20,670 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - 
2024-04-17T00:10:20,670 [INFO ] W-9000-wizardlmtest_1.0 TS_METRICS - ts_inference_latency_microseconds.Microseconds:5195308.739|#model_name:wizardlmtest,model_version:default|#hostname:ai,timestamp:1713312620
2024-04-17T00:10:20,670 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - [1m> Entering new StuffDocumentsChain chain...[0m
2024-04-17T00:10:20,670 [INFO ] W-9000-wizardlmtest_1.0 TS_METRICS - ts_queue_latency_microseconds.Microseconds:198.517|#model_name:wizardlmtest,model_version:default|#hostname:ai,timestamp:1713312620
2024-04-17T00:10:20,670 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - 
2024-04-17T00:10:20,670 [DEBUG] W-9000-wizardlmtest_1.0 org.pytorch.serve.job.RestJob - Waiting time ns: 198517, Backend time ns: 5196246967
2024-04-17T00:10:20,670 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - 
2024-04-17T00:10:20,670 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - [1m> Entering new LLMChain chain...[0m
2024-04-17T00:10:20,670 [INFO ] W-9000-wizardlmtest_1.0 TS_METRICS - QueueTime.Milliseconds:0.0|#Level:Host|#hostname:ai,timestamp:1713312620
2024-04-17T00:10:20,670 [INFO ] W-9000-wizardlmtest_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 5195
2024-04-17T00:10:20,671 [INFO ] W-9000-wizardlmtest_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:1.0|#Level:Host|#hostname:ai,timestamp:1713312620
2024-04-17T00:10:20,671 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - Prompt after formatting:
2024-04-17T00:10:20,671 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - [32;1m[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.
2024-04-17T00:10:20,671 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - 
2024-04-17T00:10:20,672 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - grade. What is SAP? – Satisfactory Academic Progress (SAP) means that you are satisfactorily progressing toward degree completion by qualitative (GPA) and quantitative (Pace and Maximum Timeframe) measures. The HSU Financial Aid Office is required by
2024-04-17T00:10:20,673 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - 
2024-04-17T00:10:20,673 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - on areas where I could improve.” -Marcus McCray, Licensed Professional Counselor-Associate, ‘23 “I really appreciated the fact that all of the professors were from such a wide array of backgrounds and theoretical orientations, so we were able to have a
2024-04-17T00:10:20,674 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - 
2024-04-17T00:10:20,674 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - information may need to be corrected. If selected, you will be directed to your Campus Logic StudentForms to complete the steps necessary for verification. You and a parent will be asked to sign a verification webform, attach any required documents, and
2024-04-17T00:10:20,674 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - 
2024-04-17T00:10:20,674 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - status) the student previously had. We can't wait to see you at HSU! Click the button below and get started on your application. If you have any questions, you can call Admissions at 877-GO-HSUTX. The best way to really experience Hardin-Simmons
2024-04-17T00:10:20,675 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - 
2024-04-17T00:10:20,675 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - Question:  What is the meaning of "HSU"?
2024-04-17T00:10:20,675 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - Helpful Answer:[0m
2024-04-17T00:10:20,675 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - 
2024-04-17T00:10:20,675 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - [1m> Finished chain.[0m
2024-04-17T00:10:20,675 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - 
2024-04-17T00:10:20,675 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - [1m> Finished chain.[0m
2024-04-17T00:10:20,675 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_LOG - Chat History Updated for User 4um2xl: [('hello', ' Hello! How may I assist you today?'), ('what does HSU stand for?', '  HSU stands for Hardin-Simmons University, a private Christian university located in Abilene, Texas.')]
2024-04-17T00:10:20,676 [INFO ] W-9000-wizardlmtest_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]HandlerTime.Milliseconds:5193.42|#ModelName:wizardlmtest,Level:Model|#type:GAUGE|#hostname:ai,1713312620,b0fa5d27-4776-495d-828e-4aefb5e71def, pattern=[METRICS]
2024-04-17T00:10:20,677 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_METRICS - HandlerTime.ms:5193.42|#ModelName:wizardlmtest,Level:Model|#hostname:ai,requestID:b0fa5d27-4776-495d-828e-4aefb5e71def,timestamp:1713312620
2024-04-17T00:10:20,677 [INFO ] W-9000-wizardlmtest_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:5193.57|#ModelName:wizardlmtest,Level:Model|#type:GAUGE|#hostname:ai,1713312620,b0fa5d27-4776-495d-828e-4aefb5e71def, pattern=[METRICS]
2024-04-17T00:10:20,677 [INFO ] W-9000-wizardlmtest_1.0-stdout MODEL_METRICS - PredictionTime.ms:5193.57|#ModelName:wizardlmtest,Level:Model|#hostname:ai,requestID:b0fa5d27-4776-495d-828e-4aefb5e71def,timestamp:1713312620
2024-04-17T00:10:29,370 [ERROR] Thread-6 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/students/mdr2104/.local/lib/python3.10/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/students/mdr2104/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/students/mdr2104/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 66, in gpu_utilization
    from nvgpu import list_gpus
  File "/home/students/mdr2104/.local/lib/python3.10/site-packages/nvgpu/list_gpus.py", line 3, in <module>
    import arrow
ModuleNotFoundError: No module named 'arrow'

2024-04-17T00:11:29,379 [ERROR] Thread-7 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/students/mdr2104/.local/lib/python3.10/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/students/mdr2104/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/students/mdr2104/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 66, in gpu_utilization
    from nvgpu import list_gpus
  File "/home/students/mdr2104/.local/lib/python3.10/site-packages/nvgpu/list_gpus.py", line 3, in <module>
    import arrow
ModuleNotFoundError: No module named 'arrow'

2024-04-17T00:12:29,369 [ERROR] Thread-8 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/students/mdr2104/.local/lib/python3.10/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/students/mdr2104/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/students/mdr2104/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 66, in gpu_utilization
    from nvgpu import list_gpus
  File "/home/students/mdr2104/.local/lib/python3.10/site-packages/nvgpu/list_gpus.py", line 3, in <module>
    import arrow
ModuleNotFoundError: No module named 'arrow'

2024-04-17T00:13:29,369 [ERROR] Thread-9 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/students/mdr2104/.local/lib/python3.10/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/students/mdr2104/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/students/mdr2104/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 66, in gpu_utilization
    from nvgpu import list_gpus
  File "/home/students/mdr2104/.local/lib/python3.10/site-packages/nvgpu/list_gpus.py", line 3, in <module>
    import arrow
ModuleNotFoundError: No module named 'arrow'

2024-04-17T00:14:29,369 [ERROR] Thread-10 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/students/mdr2104/.local/lib/python3.10/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/students/mdr2104/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/students/mdr2104/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 66, in gpu_utilization
    from nvgpu import list_gpus
  File "/home/students/mdr2104/.local/lib/python3.10/site-packages/nvgpu/list_gpus.py", line 3, in <module>
    import arrow
ModuleNotFoundError: No module named 'arrow'

2024-04-17T00:15:29,369 [ERROR] Thread-11 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/students/mdr2104/.local/lib/python3.10/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/students/mdr2104/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/students/mdr2104/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 66, in gpu_utilization
    from nvgpu import list_gpus
  File "/home/students/mdr2104/.local/lib/python3.10/site-packages/nvgpu/list_gpus.py", line 3, in <module>
    import arrow
ModuleNotFoundError: No module named 'arrow'

2024-04-17T00:16:29,368 [ERROR] Thread-12 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/students/mdr2104/.local/lib/python3.10/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/students/mdr2104/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/students/mdr2104/.local/lib/python3.10/site-packages/ts/metrics/system_metrics.py", line 66, in gpu_utilization
    from nvgpu import list_gpus
  File "/home/students/mdr2104/.local/lib/python3.10/site-packages/nvgpu/list_gpus.py", line 3, in <module>
    import arrow
ModuleNotFoundError: No module named 'arrow'

