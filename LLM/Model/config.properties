inference_address=http://0.0.0.0:8080
management_address=http://0.0.0.0:8081
metrics_address=http://0.0.0.0:8082
enable_envvars_config=true
#batch_size=10
#max_batch_delay=50
# If using GPUs, adjust accordingly
number_of_gpu=1
# For CPU optimization
cpu_launcher_enable=true
cpu_launcher_args=--use_logical_core
# The following command will register a model "wizardlmtest.mar" and configure TorchServe to use a batch_size of 8 and a max batch delay of 50 milli seconds, in the config.properties.
#models={\
#  "wizardlmtest": {\
#    "1.0": {\
#        "defaultVersion": true,\
#        "marName": "wizardlmtest.mar",\
#        "minWorkers": 1,\
#        "maxWorkers": 1,\
#        "batchSize": 8,\
#        "maxBatchDelay": 50,\
#        "responseTimeout": 120\
#    }\
#  }\
#}